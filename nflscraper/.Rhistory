#
#
# b.frame.root <- sqrt(b.frame.sq)
# b.frame.root2 <- data.frame(cbind(b.frame.root[,1],
#                                   -1*b.frame.root[,2],
#                                   -1*b.frame.root[,3]))
# b.frame.root3 <- data.frame(cbind(b.frame.root[,1:2],
#                                   -1*b.frame.root[,3]))
# b.frame.root4 <- data.frame(cbind(b.frame.root[,1],
#                                   -1*b.frame.root[,2],
#                                   b.frame.root[,3]))
# b.frame.root5 <- -1*b.frame.root
# b.frame.root6 <- data.frame(cbind(-1*b.frame.root[,1],
#                                   b.frame.root[,2],
#                                   b.frame.root[,3]))
# b.frame.root7 <- data.frame(cbind(-1*b.frame.root[,1],
#                                   -1*b.frame.root[,2],
#                                   b.frame.root[,3]))
# b.frame.root8 <- data.frame(cbind(-1*b.frame.root[,1],
#                                   b.frame.root[,2],
#                                   -1*b.frame.root[,3]))
# b.frame <- b.frame.root
# b.frame <- b.frame.root2
# names(b.frame.root8) <- c("X1","X2","X3")
# b.frame <- rbind(b.frame.root,b.frame.root2,
#                  b.frame.root3,b.frame.root4,
#                  b.frame.root5,b.frame.root6,
#                  b.frame.root7,b.frame.root8)
#######################################################
#Define beta
beta.val <- seq(0,1,length.out = 101)
Deltat <- 1
min.err <- min.err.neg <- NULL
inner.sum <- inner.sum.neg <- NULL
inf.cond <- inf.cond.neg <- NULL
pls.k.pos <- pls.k.neg <- NULL
pc.k.pos <- pc.k.neg <- NULL
hatpsir.k.pos <- hatpsir.k.neg <- matrix(NA,nrow=num.points,ncol=101)
hatpsipls.k.pos <- hatpsipls.k.neg <- matrix(NA,nrow=num.points,ncol=101)
#Define a flag to control what r will be (chosen by cross-validation, or not)
#If fixr is TRUE, then we fix r at a level; if not, we choose by CV.
fixr <- TRUE
fixrval <- 3
pt <- proc.time()
for (k in 1:2){
for (l in 1:101){
#Multiply by beta
if(k == 1){
Ly.real <- beta.val[l]*w.scal.train + sqrt((1-beta.val[l]^2))*cos.scal.train
}
if (k == 2){
Ly.real <- beta.val[l]*w.scal.train - sqrt((1-beta.val[l]^2))*cos.scal.train
}
train.mat <- as.matrix(Ly.real[,])
XXdata <- train.mat
#Get the indices of each here
indClass0tmp=which(y.train==0)
indClass1tmp=which(y.train==1)
#Get the number in each sample
n0=length(indClass0tmp)
n1=length(indClass1tmp)
##########################################################################
#Divide X matrices
#Get separate XX matrices for the training 0 and training 1 data.
XXtrain0=XXdata[indClass0tmp,]
XXtrain1=XXdata[indClass1tmp,]
#Make one matrix that contains all training data.
XXtrain=rbind(XXtrain0,XXtrain1)
#indices of class label 0 or 1
indClass0=1:n0
indClass1=1:n1+n0
#Define outcome Y for the training data.
Y=matrix(0,nrow=ntrain,ncol=1)
Y[indClass1]=rep(1,n1)
# Rescale the data in some way to avoid too many numerical problems. For ex., do it this way:
# varXX=var(as.vector(XXtrain))
# muX=mean(as.vector(XXtrain))
# XXtrain=(XXtrain-muX)/sqrt(varXX)
# XXtest=(XXtest-muX)/sqrt(varXX)
if(k == 1){
Ly.real.test  <- beta.val[l]*w.scal.test + sqrt((1-beta.val[l]^2))*cos.scal.test
}
if(k == 2){
Ly.real.test  <- beta.val[l]*w.scal.test - sqrt((1-beta.val[l]^2))*cos.scal.test
}
test.mat <- as.matrix(Ly.real.test[,])
XXtest <- test.mat
##########################################################################
#Computations useful for later
# Calculate means in each class (odd way of doing it but how Delaigle did it.)
barXClass0=XXtrain0[1,]
for(i in 2:n0)
barXClass0=barXClass0+XXtrain0[i,]
barXClass1=XXtrain1[1,]
for(i in 2:n1)
barXClass1=barXClass1+XXtrain1[i,]
#Divide by n's to get the means.
barXClass0=barXClass0/n0
barXClass1=barXClass1/n1
##########################################################################
#Construct PC basis estimators based on the data from the two groups centered to their mean
ZZ=XXtrain0-outer(rep(1, n0), barXClass0)
ZZ=rbind(ZZ,XXtrain1-outer(rep(1, n1), barXClass1))
compo=prcomp(ZZ)
phi=(compo$rotation)
phi=phi/sqrt(Deltat)
lambda=(compo$sdev)
lambda=lambda^2*Deltat
# npsir is the maximum number of terms considered in our sums
cumul=cumsum(lambda)/sum(lambda)
npsir=min(which(cumul>1-1/ntrain^2))
npsir=min(npsir,ntrain/2)
#Difference between means + projections of that difference on the basis fucntions. Plays the role of the mu_j's in the paper
DiffbarX=barXClass1-barXClass0
hatmuj=DiffbarX%*%phi[,]*Deltat
#####################################################################
#Insert actual test code
CV=rep(0,npsir)
#nbcv is the number of local minima found so far
nbCV=0
#npsirA will be the number of components we keep
npsirA=1
if (fixr==FALSE){
#B5fold = Number of data partitions created to compute the cross-validation criterion
B=200
K=5
KKCV=round(ntrain/K)
#ind5foldCV: indices of the samples Xstarb1,...,Xstarbm for b=1..B
ind5foldCV=matrix(0,nrow=B,ncol=KKCV)
for(b in 1:B)
{
s=runif(KKCV,0,ntrain)
s=ceiling(s)
ind5foldCV[b,]=s
}
#To save time later, compute now the eigen functions and eigen values for each of the CV samples and save them in matirces and vectors
for(b in 1:B)
{
i=ind5foldCV[b,]
XXCV=ZZ[-i,]
compoCV=prcomp(XXCV)
phiCV=(compo$rotation)
eval(parse(text=paste("phiCV",b,"=phiCV/sqrt(Deltat)", sep = "")))
lambdaCV=(compoCV$sdev)
eval(parse(text=paste("lambdaCV",b,"=lambdaCV^2*Deltat", sep = "")))
}
for(j in 1:npsir)
{
if(nbCV<2)
{
CV[j]=CVpsi(XXtrain,j)
if(j>2)
{
#check your CV function to see if CV is flat in some parts. If it is flat you can only find the mins if you use a non strict inequality
if((CV[j-1]<CV[j-2])&(CV[j-1]<=CV[j]))
{
nbCV=nbCV+1
if(nbCV<2)
{
npsirA=j-1
minCV=CV[j-1]
}
if((nbCV==2)&(CV[j-1]<minCV))
{	npsirA=j-1
minCV=CV[j-1]
}
}#end if CV[j-1]
}#end if j>1
}#end while j
}#end for j
}
else{
npsirA <- fixrval
}
#Compute the function hatpsir
hatpsir=0*phi[,1]
for(j in 1:npsirA)
hatpsir=hatpsir+hatmuj[j]/lambda[j]*phi[,j]
#--------------------------------------------------------------------------
#--------------------------------------------------------------------------
# METHOD 2: Estimate psir  computed through MPLSR (partial least squares)
#--------------------------------------------------------------------------
#--------------------------------------------------------------------------
#Compute number of components to keep by CV
CV=rep(0,npsir)
#m will be the number of components we keep
if (fixr==FALSE){
m=1
for(j in 1:npsir)
{
if(nbCV<2)
{
CV[j]=CVmplsr(XXtrain,Y,j)
if(j>2)
{
#check your CV function to see if CV is flat in some parts. If it is flat you can only find the mins if you use a non strict inequality
if((CV[j-1]<CV[j-2])&(CV[j-1]<=CV[j]))
{
nbCV=nbCV+1
if(nbCV<2)
{
m=j-1
minCV=CV[j-1]
}
if((nbCV==2)&(CV[j-1]<minCV))
{	m=j-1
minCV=CV[j-1]
}
}#end if CV[j-1]
}#end if j>1
}#end while j
}#end for j
}
else{
m <- fixrval
}
#Compute the resulting beta_r
hatpsipls=mplsr(XXtrain,Y,m)$COEF
#--------------------------------------------------------------------------------------------------------------------------------------------------
#--------------------------------------------------------------------------------------------------------------------------------------------------
# Apply the centroid classifier to the test data using each of the functions psir and compute the number of errors made by the classifier
#--------------------------------------------------------------------------------------------------------------------------------------------------
#--------------------------------------------------------------------------------------------------------------------------------------------------
#Will contain labels of classes to which each of the Xtest data are assigned byb the classifier
ClassTestPC=rep(0,ntest)
ClassTestPLS=rep(0,ntest)
for(b in 1:ntest)
{
#Test curve
X=XXtest[b,]
#project the data on the estimated psir function (PCA method)
Xproj=X%*%hatpsir
XXtrainproj=XXtrain%*%hatpsir
#Compute statistic used to classify the data
TofX=(Xproj-mean(XXtrainproj[indClass1]))^2-(Xproj-mean(XXtrainproj[indClass0]))^2
#Compute class label
if(TofX<0)
ClassTestPC[b]=1
#project the data on the estimated function computed by PLS
Xproj=X%*%hatpsipls*Deltat
XXtrainproj=XXtrain%*%hatpsipls*Deltat
#Compute statistic used to classify the data
TofX=(Xproj-mean(XXtrainproj[indClass1]))^2-(Xproj-mean(XXtrainproj[indClass0]))^2
#Compute class label
if(TofX<0)
ClassTestPLS[b]=1
}
if(k == 1){
#Real results
pls.k.pos <- c(pls.k.pos,sum(diag(table(ClassTestPLS,y.test))) / ntest)
pc.k.pos <- c(pc.k.pos,sum(diag(table(ClassTestPC,y.test))) / ntest)
#Asymptotic bounds
min.err <- c(min.err,1 - pnorm(.5 * sqrt(sum((as.vector(hatmuj))^2/lambda))))
inner.sum <- c(inner.sum,sum(lambda^(-1) * hatmuj^2))
inf.cond <- c(inf.cond,sum(lambda^(-2) * hatmuj^2 ))
#Save the psi function
hatpsir.k.pos[,l] <- hatpsir
hatpsipls.k.pos[,l] <- hatpsipls
}
if (k == 2){
#Real results
pls.k.neg <- c( pls.k.neg,sum(diag(table(ClassTestPLS,y.test)))/ ntest )
pc.k.neg <-  c( pc.k.neg,sum(diag(table(ClassTestPC,y.test)))/ ntest )
#Asymptotic bounds
min.err.neg <- c(min.err.neg,1 - pnorm(.5 * sqrt(sum((as.vector(hatmuj))^2/lambda))))
inner.sum.neg <- c(inner.sum.neg,sum(lambda^(-1) * hatmuj^2))
inf.cond.neg <- c(inf.cond.neg,sum(lambda^(-2) * hatmuj^2 ))
#Save the psi function
hatpsir.k.neg[,l] <- hatpsir
hatpsipls.k.neg[,l] <- hatpsipls
}
#####################################################################
}
}
par(mfrow = c(2,2))
plot(x = beta.val,y=pc.k.pos,type="l",main="PC, K positive",ylim=c(0,1))
lines(x=beta.val,y=1-min.err,type="l",col="blue")
plot(x = beta.val,y=pls.k.pos,type="l",main="PLS, K positive",ylim=c(0,1))
lines(x=beta.val,y=1-min.err,type="l",col="blue")
plot(x = beta.val,y=pc.k.neg,type="l",main="PC, K negative",ylim=c(0,1))
lines(x=beta.val,y=1-min.err.neg,type="l",col="blue")
plot(x = beta.val,y=pls.k.neg,type="l",main="PLS, K negative",ylim=c(0,1))
lines(x=beta.val,y=1-min.err.neg,type="l",col="blue")
par(mfrow = c(1,1))
map1 <- melt(hatpsipls.k.pos)
ggplot(data = map1, aes(x=Var2, y=Var1, fill=value)) +
geom_tile() + scale_fill_gradientn(colours = terrain.colors(10)) +
labs(title = "PLS Psi function vs. Beta (K pos)", x = "Index of Beta",y ="Psi function")
map2 <- melt(hatpsipls.k.neg)
ggplot(data = map2, aes(x=Var2, y=Var1, fill=value)) +
geom_tile() + scale_fill_gradientn(colours = terrain.colors(10)) +
labs(title = "PLS Psi function vs. Beta (K neg)", x = "Index of Beta",y ="Psi function")
map3 <- melt(hatpsir.k.pos)
ggplot(data = map3, aes(x=Var2, y=Var1, fill=value)) +
geom_tile() + scale_fill_gradientn(colours = terrain.colors(10)) +
labs(title = "PC Psi function vs. Beta (K pos)", x = "Index of Beta",y ="Psi function")
map4 <- melt(hatpsir.k.neg)
ggplot(data = map3, aes(x=Var2, y=Var1, fill=value)) +
geom_tile() + scale_fill_gradientn(colours = terrain.colors(10)) +
labs(title = "PC Psi function vs. Beta (K neg)", x = "Index of Beta",y ="Psi function")
pdf("sim-err-curves.pdf")
par(mfrow = c(2,2))
plot(x = beta.val,y=pc.k.pos,type="l",main="PC, K positive",ylim=c(0,1))
lines(x=beta.val,y=1-min.err,type="l",col="blue")
plot(x = beta.val,y=pls.k.pos,type="l",main="PLS, K positive",ylim=c(0,1))
lines(x=beta.val,y=1-min.err,type="l",col="blue")
dev.off()
pdf(file="sim-err-curves.pdf",height=8,width=6)
par(mfrow = c(2,2))
plot(x = beta.val,y=pc.k.pos,type="l",main="PC, K positive",ylim=c(0,1))
lines(x=beta.val,y=1-min.err,type="l",col="blue")
dev.off()
pdf(file="sim-err-curves.pdf",height=8,width=6)
par(mfrow = c(2,2))
plot(x = beta.val,y=pc.k.pos,type="l",main="PC, K positive",ylim=c(0,1))
lines(x=beta.val,y=1-min.err,type="l",col="blue")
plot(x = beta.val,y=pls.k.pos,type="l",main="PLS, K positive",ylim=c(0,1))
lines(x=beta.val,y=1-min.err,type="l",col="blue")
plot(x = beta.val,y=pc.k.neg,type="l",main="PC, K negative",ylim=c(0,1))
lines(x=beta.val,y=1-min.err.neg,type="l",col="blue")
plot(x = beta.val,y=pls.k.neg,type="l",main="PLS, K negative",ylim=c(0,1))
lines(x=beta.val,y=1-min.err.neg,type="l",col="blue")
par(mfrow = c(1,1))
dev.off()
pdf(file="sim-err-curves.pdf",height=8,width=6)
par(mfrow = c(2,2))
plot(x = beta.val,y=pc.k.pos,type="l",main="PC, K positive",ylim=c(0,1))
lines(x=beta.val,y=1-min.err,type="l",col="blue")
plot(x = beta.val,y=pls.k.pos,type="l",main="PLS, K positive",ylim=c(0,1))
lines(x=beta.val,y=1-min.err,type="l",col="blue")
plot(x = beta.val,y=pc.k.neg,type="l",main="PC, K negative",ylim=c(0,1))
lines(x=beta.val,y=1-min.err.neg,type="l",col="blue")
plot(x = beta.val,y=pls.k.neg,type="l",main="PLS, K negative",ylim=c(0,1))
lines(x=beta.val,y=1-min.err.neg,type="l",col="blue")
par(mfrow = c(1,1))
dev.off()
require(devtools)
require(baseballr)
library(Lahman)
library(plyr)
library(dplyr)
pitches.April17 <- scrape_statcast_savant_pitcher_all("2017-04-02", "2017-04-30")
pitches.April17 <- scrape_statcast_savant_pitcher_all("2017-04-02", "2017-04-30")
pitches.May17 <- scrape_statcast_savant_pitcher_all("2017-05-01", "2017-05-31")
pitches.June17 <- scrape_statcast_savant_pitcher_all("2017-06-01", "2017-06-30")
pitches.July17 <- scrape_statcast_savant_pitcher_all("2017-07-01", "2017-07-31")
pitches.August17 <- scrape_statcast_savant_pitcher_all("2017-08-01", "2017-08-31")
pitches.September17 <- scrape_statcast_savant_pitcher_all("2017-09-01", "2017-09-30")
pitches.October17 <- scrape_statcast_savant_pitcher_all("2017-10-01", "2017-10-01")
pitches.April17 <- pitches.April17[, c(1, 2, 9, 53:57, 79)]
pitches.May17 <- pitches.May17[, c(1, 2, 9, 53:57, 79)]
pitches.June17 <- pitches.June17[, c(1, 2, 9, 53:57, 79)]
pitches.July17 <- pitches.July17[, c(1, 2, 9, 53:57, 79)]
pitches.August17 <- pitches.August17[, c(1, 2, 9, 53:57, 79)]
pitches.September17 <- pitches.September17[, c(1, 2, 9, 53:57, 79)]
pitches.October17 <- pitches.October17[, c(1, 2, 9, 53:57, 79)]
April17.hr <- pitches.April17 %>%
filter(events == "home_run")
May17.hr <- pitches.May17 %>%
filter(events == "home_run")
June17.hr <- pitches.June17 %>%
filter(events == "home_run")
July17.hr <- pitches.July17 %>%
filter(events == "home_run")
August17.hr <- pitches.August17 %>%
filter(events == "home_run")
September17.hr <- pitches.September17 %>%
filter(events == "home_run")
October17.hr <- pitches.October17 %>%
filter(events == "home_run")
#2016
pitches.April16 <- scrape_statcast_savant_pitcher_all("2016-04-03", "2016-04-30")
pitches.May16 <- scrape_statcast_savant_pitcher_all("2016-05-01", "2016-05-31")
pitches.June16 <- scrape_statcast_savant_pitcher_all("2016-06-01", "2016-06-30")
pitches.July16 <- scrape_statcast_savant_pitcher_all("2016-07-01", "2016-07-31")
pitches.August16 <- scrape_statcast_savant_pitcher_all("2016-08-01", "2016-08-31")
pitches.September16 <- scrape_statcast_savant_pitcher_all("2016-09-01", "2016-09-30")
pitches.October16 <- scrape_statcast_savant_pitcher_all("2016-10-01", "2016-10-02")
pitches.April16 <- pitches.April16[, c(1, 2, 9, 53:57, 79)]
pitches.May16 <- pitches.May16[, c(1, 2, 9, 53:57, 79)]
pitches.June16 <- pitches.June16[, c(1, 2, 9, 53:57, 79)]
pitches.July16 <- pitches.July16[, c(1, 2, 9, 53:57, 79)]
pitches.August16 <- pitches.August16[, c(1, 2, 9, 53:57, 79)]
pitches.September16 <- pitches.September16[, c(1, 2, 9, 53:57, 79)]
pitches.October16 <- pitches.October16[, c(1, 2, 9, 53:57, 79)]
April16.hr <- pitches.April16 %>%
filter(events == "home_run")
May16.hr <- pitches.May16 %>%
filter(events == "home_run")
June16.hr <- pitches.June16 %>%
filter(events == "home_run")
July16.hr <- pitches.July16 %>%
filter(events == "home_run")
August16.hr <- pitches.August16 %>%
filter(events == "home_run")
September16.hr <- pitches.September16 %>%
filter(events == "home_run")
October16.hr <- pitches.October16 %>%
filter(events == "home_run")
head(April16.hr)
head(pitches.April16)
library(devtools)
library(nflscrapR)
library(tidyverse)
library(ggjoy)
games2014 <- season_games(Season = 2014)
head(games2014)
sea.games <- games2014 %>%
filter(home == "SEA" | away = "SEA")
sea.games <- games2014 %>%
filter(home == "SEA" | away == "SEA")
head(sea.games)
help(nflscrapR)
??nflscrapR
# Look at super bowl 47
superbowl47 <- game_play_by_play(GameID = 2013020300)
# Explore dataframe dimensions
dim(superbowl47)
warnings()
superbowl47 %>%
group_by(posteam) %>%
summarize(offensiveplays = n()) %>%
filter(., posteam != "")
library(ggplot2)
library(gridExtra)
sb_team_summary_stats <- superbowl47 %>% group_by(posteam) %>%
summarize(offensiveplays = n(),
avg.yards.gained = mean(Yards.Gained,
na.rm = TRUE),
pointsperplay = max(PosTeamScore, na.rm = TRUE) / n(),
playduration = mean(PlayTimeDiff)) %>%
filter(., posteam != "") %>%
as.data.frame()
head(sb_team_summary_stats)
# Yards per play plot
plot_yards <- ggplot(sb_team_summary_stats,
aes(x = posteam, y = avg.yards.gained)) +
geom_bar(aes(fill = posteam), stat = "identity") +
geom_label(aes(x = posteam, y = avg.yards.gained + .3,
label = round(avg.yards.gained,2)),
size = 4, fontface = "bold") +
labs(title = "Superbowl 47: Yards per Play by Team",
x = "Teams", y = "Average Yards per Play") +
scale_fill_manual(values = c("#241773", "#B3995D")) +
theme(plot.title = element_text(hjust = .5, face = "bold"))
# Points per play plot
plot_points <- ggplot(sb_team_summary_stats,
aes(x = posteam, y = pointsperplay)) +
geom_bar(aes(fill = posteam), stat = "identity") +
geom_label(aes(x = posteam, y = pointsperplay + .05,
label = round(pointsperplay,5)),
size = 4, fontface = "bold") +
labs(title = "Superbowl 47: Points per Play by Team",
x = "Teams", y = "Points per Play") +
scale_fill_manual(values = c("#241773", "#B3995D")) +
theme(plot.title = element_text(hjust = .5, face = "bold"))
# Play duration plot
plot_time <- ggplot(sb_team_summary_stats,
aes(x = posteam, y = playduration)) +
geom_bar(aes(fill = posteam), stat = "identity") +
geom_label(aes(x = posteam, y = playduration + .05,
label = round(playduration,2)),
size = 4, fontface = "bold") +
labs(title = "Superbowl 47: Average Play Time Duration \n by Team",
x = "Teams", y = "Average Play Duration") +
scale_fill_manual(values = c("#241773", "#B3995D"))+
theme(plot.title = element_text(hjust = .5, face = "bold"))
# Plotting the three charts together
grid.arrange(plot_yards, plot_points, plot_time, ncol =2)
pbp_2010 <- season_play_by_play(2010)
head(pbp_2010)
# Get play-by-play data but this seems to not work.
pbp_2009 <- season_play_by_play(2009)
pbp_2011 <- season_play_by_play(2011)
pbp_2012 <- season_play_by_play(2012)
pbp_2013 <- season_play_by_play(2013)
pbp_2014 <- season_play_by_play(2014)
pbp_2015 <- season_play_by_play(2015)
pbp_2016 <- season_play_by_play(2016)
pbp_data <- bind_rows(pbp_2009, pbp_2010, pbp_2011, pbp_2012,
pbp_2013, pbp_2014, pbp_2015, pbp_2016)
getwd()
setwd("~/Desktop/ucsb-year4/SAG tutorials/nflscraper")
saveRDS(pbp_data, file = "pbp_nfl.Rds")
# Get passing stats from play-by-play data
passing_stats <- pbp_data %>%
filter(PassAttempt == 1 & PlayType != "No Play" & !is.na(Passer)) %>%
group_by(Season, Passer) %>%
summarise(Attempts = n(),
Total_EPA = sum(EPA, na.rm = TRUE),
EPA_per_Att = Total_EPA/Attempts) %>%
filter(Attempts >= 50)
# Make a ggplot of it
ggplot(passing_stats, aes(x = EPA_per_Att, y = as.factor(Season))) +
geom_joy(scale = 3, rel_min_height = 0.01) +
theme_joy() + ylab("Season") + xlab("EPA per Pass Attempt") +
scale_y_discrete(expand = c(0.01, 0)) +
scale_x_continuous(expand = c(0.01, 0)) +
ggtitle("The Shifting Distribution of EPA per Pass Attempt") +
theme(plot.title = element_text(hjust = 0.5, size = 16),
axis.title = element_text(size = 16),
axis.text = element_text(size = 16))
head(passing_stats)
head(pbp_2009)
passing_stats %>% filter(Passer == "A. Rodgers")
passing_stats %>% filter(Passer == "A.Rodgers")
devtools::install_github("abresler/nbastatR")
library(nbastatR)
library("nbastatR")
install.packages("tidyr")
library(tidyr)
library("nbastatR")
